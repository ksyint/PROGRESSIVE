model:
  model_name: "llava-hf/vip-llava-7b-hf"
  dtype: "bfloat16"
  low_cpu_mem_usage: true

data:
  train_json: "data/vqa_cot_dataset.json"

training:
  output_dir: "output"
  epochs: 100
  batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 2.0e-5
  min_lr: 1.0e-6
  warmup_ratio: 0.03
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.98
  max_grad_norm: 1.0
  seed: 42
  save_every_epochs: 5

curriculum:
  hard_transition_epoch: 19  
  easy_warmup_epochs: 4
  plateau_patience: 5
  ema_rate: 0.9
  ramp_kappa: 10
  gamma: 0.5
  tau: 0.1
  gamma_H: 0.6
  epsilon_plat: 0.01
  delta_rise: 0.05
  hard_step_up: 0.05
  hard_step_down: 0.1
  hard_budget_max: 0.3
  hard_budget_init: 0.0

losses:
  lambda_ans: 0.8
  lambda_cot: 0.2
  
  
logging:
  log_dir: "output/logs"
